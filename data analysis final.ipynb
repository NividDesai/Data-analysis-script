{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\desai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\desai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 800\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid choice. Please try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 800\u001b[0m     interactive_menu()\n",
      "Cell \u001b[1;32mIn[1], line 740\u001b[0m, in \u001b[0;36minteractive_menu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    737\u001b[0m target_language \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the target language code (e.g., \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for English): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    739\u001b[0m \u001b[38;5;66;03m# Fetch transcripts\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m transcripts \u001b[38;5;241m=\u001b[39m fetch_and_translate_transcripts(video_ids, target_language)\n\u001b[0;32m    741\u001b[0m save_transcripts_to_file(transcripts)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;66;03m# Fetch video metadata\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 186\u001b[0m, in \u001b[0;36mfetch_and_translate_transcripts\u001b[1;34m(video_ids, target_language)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_id \u001b[38;5;129;01min\u001b[39;00m video_ids:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;66;03m# Fetch transcript\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m         available_transcripts \u001b[38;5;241m=\u001b[39m YouTubeTranscriptApi\u001b[38;5;241m.\u001b[39mlist_transcripts(video_id)\n\u001b[0;32m    187\u001b[0m         transcript \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m target_language \u001b[38;5;129;01min\u001b[39;00m [t\u001b[38;5;241m.\u001b[39mlanguage_code \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m available_transcripts]:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\youtube_transcript_api\\_api.py:71\u001b[0m, in \u001b[0;36mYouTubeTranscriptApi.list_transcripts\u001b[1;34m(cls, video_id, proxies, cookies)\u001b[0m\n\u001b[0;32m     69\u001b[0m     http_client\u001b[38;5;241m.\u001b[39mcookies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_cookies(cookies, video_id)\n\u001b[0;32m     70\u001b[0m http_client\u001b[38;5;241m.\u001b[39mproxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;28;01mif\u001b[39;00m proxies \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TranscriptListFetcher(http_client)\u001b[38;5;241m.\u001b[39mfetch(video_id)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:48\u001b[0m, in \u001b[0;36mTranscriptListFetcher.fetch\u001b[1;34m(self, video_id)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TranscriptList\u001b[38;5;241m.\u001b[39mbuild(\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_client,\n\u001b[0;32m     47\u001b[0m         video_id,\n\u001b[1;32m---> 48\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_captions_json(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_video_html(video_id), video_id),\n\u001b[0;32m     49\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:82\u001b[0m, in \u001b[0;36mTranscriptListFetcher._fetch_video_html\u001b[1;34m(self, video_id)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetch_video_html\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id):\n\u001b[1;32m---> 82\u001b[0m     html \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_html(video_id)\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://consent.youtube.com/s\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m html:\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_consent_cookie(html, video_id)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:91\u001b[0m, in \u001b[0;36mTranscriptListFetcher._fetch_html\u001b[1;34m(self, video_id)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetch_html\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id):\n\u001b[1;32m---> 91\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_client\u001b[38;5;241m.\u001b[39mget(WATCH_URL\u001b[38;5;241m.\u001b[39mformat(video_id\u001b[38;5;241m=\u001b[39mvideo_id), headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccept-Language\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men-US\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unescape(_raise_http_errors(response, video_id)\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\requests\\sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\requests\\sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    266\u001b[0m         req,\n\u001b[0;32m    267\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    268\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    269\u001b[0m         verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[0;32m    270\u001b[0m         cert\u001b[38;5;241m=\u001b[39mcert,\n\u001b[0;32m    271\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    272\u001b[0m         allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[0;32m    274\u001b[0m     )\n\u001b[0;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     r\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\urllib3\\response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\urllib3\\response.py:1209\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_chunk(amt)\n\u001b[0;32m   1210\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   1211\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m )\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\site-packages\\urllib3\\response.py:1155\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# amt > self.chunk_left\u001b[39;00m\n\u001b[1;32m-> 1155\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# type: ignore[union-attr] # Toss the CRLF at the end of the chunk.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\http\\client.py:640\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[0;32m    634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Aivancity\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from googletrans import Translator\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "translator = Translator()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "output_file = \"transcript_results.txt\"\n",
    "metadata_file = \"video_metadata.csv\"\n",
    "\n",
    "# YouTube Data API setup\n",
    "# Replace with your actual API key\n",
    "DEVELOPER_KEY = \"\"\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "def get_youtube_service():\n",
    "    \"\"\"Initialize YouTube API client\"\"\"\n",
    "    try:\n",
    "        youtube = googleapiclient.discovery.build(\n",
    "            YOUTUBE_API_SERVICE_NAME, \n",
    "            YOUTUBE_API_VERSION, \n",
    "            developerKey=DEVELOPER_KEY\n",
    "        )\n",
    "        return youtube\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing YouTube API: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def fetch_video_metadata(video_ids):\n",
    "    \"\"\"\n",
    "    Retrieve detailed metadata for given YouTube video IDs\n",
    "    \n",
    "    Args:\n",
    "        video_ids (list): List of YouTube video IDs\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of video metadata\n",
    "    \"\"\"\n",
    "    youtube = get_youtube_service()\n",
    "    video_metadata = {}\n",
    "\n",
    "    try:\n",
    "        # Retrieve video details\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,statistics,contentDetails,status\",\n",
    "            id=\",\".join(video_ids)\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for video in response.get('items', []):\n",
    "            video_id = video['id']\n",
    "            metadata = {\n",
    "                'title': video['snippet'].get('title', 'N/A'),\n",
    "                'channel_title': video['snippet'].get('channelTitle', 'N/A'),\n",
    "                'published_at': video['snippet'].get('publishedAt', 'N/A'),\n",
    "                'description': video['snippet'].get('description', 'N/A'),\n",
    "                'tags': video['snippet'].get('tags', []),\n",
    "                'view_count': int(video['statistics'].get('viewCount', 0)),\n",
    "                'like_count': int(video['statistics'].get('likeCount', 0)),\n",
    "                'comment_count': int(video['statistics'].get('commentCount', 0)),\n",
    "                'duration': video['contentDetails'].get('duration', 'N/A'),\n",
    "                'privacy_status': video['status'].get('privacyStatus', 'N/A')\n",
    "            }\n",
    "            video_metadata[video_id] = metadata\n",
    "\n",
    "        # Fetch and add comments (limited to most recent comments)\n",
    "        comments_metadata = fetch_video_comments(youtube, video_ids)\n",
    "        for video_id, comments in comments_metadata.items():\n",
    "            video_metadata[video_id]['comments'] = comments\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching video metadata: {e}\")\n",
    "\n",
    "    return video_metadata\n",
    "\n",
    "def fetch_video_comments(youtube, video_ids, max_comments_per_video=10):\n",
    "    \"\"\"\n",
    "    Retrieve comments for given video IDs\n",
    "    \n",
    "    Args:\n",
    "        youtube (obj): YouTube API service object\n",
    "        video_ids (list): List of YouTube video IDs\n",
    "        max_comments_per_video (int): Maximum number of comments to retrieve per video\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of comments for each video\n",
    "    \"\"\"\n",
    "    comments_metadata = {}\n",
    "\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            comments_request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=max_comments_per_video,\n",
    "                order=\"relevance\"\n",
    "            )\n",
    "            comments_response = comments_request.execute()\n",
    "\n",
    "            comments = [\n",
    "                {\n",
    "                    'text': comment['snippet']['topLevelComment']['snippet']['textDisplay'],\n",
    "                    'author': comment['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "                    'like_count': comment['snippet']['topLevelComment']['snippet'].get('likeCount', 0),\n",
    "                    'published_at': comment['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "                }\n",
    "                for comment in comments_response.get('items', [])\n",
    "            ]\n",
    "            comments_metadata[video_id] = comments\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "            comments_metadata[video_id] = []\n",
    "\n",
    "    return comments_metadata\n",
    "\n",
    "def save_metadata_to_file(video_metadata):\n",
    "    \"\"\"\n",
    "    Save video metadata to a CSV file\n",
    "    \n",
    "    Args:\n",
    "        video_metadata (dict): Dictionary of video metadata\n",
    "    \"\"\"\n",
    "    # Prepare data for DataFrame\n",
    "    metadata_list = []\n",
    "    for video_id, metadata in video_metadata.items():\n",
    "        row = {\n",
    "            'video_id': video_id,\n",
    "            **{k: v for k, v in metadata.items() if k not in ['comments', 'tags']}\n",
    "        }\n",
    "        metadata_list.append(row)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(metadata_list)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(metadata_file, index=False, encoding='utf-8')\n",
    "    print(f\"Metadata saved to {metadata_file}\")\n",
    "\n",
    "def analyze_video_metrics(video_metadata):\n",
    "    \"\"\"\n",
    "    Provide basic analysis of video metrics\n",
    "    \n",
    "    Args:\n",
    "        video_metadata (dict): Dictionary of video metadata\n",
    "    \"\"\"\n",
    "    print(\"\\nVideo Metrics Analysis:\")\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    total_views = sum(metadata['view_count'] for metadata in video_metadata.values())\n",
    "    total_likes = sum(metadata['like_count'] for metadata in video_metadata.values())\n",
    "    total_comments = sum(metadata['comment_count'] for metadata in video_metadata.values())\n",
    "    \n",
    "    # Calculate averages\n",
    "    video_count = len(video_metadata)\n",
    "    avg_views = total_views / video_count\n",
    "    avg_likes = total_likes / video_count\n",
    "    avg_comments = total_comments / video_count\n",
    "    \n",
    "    print(f\"Total Videos: {video_count}\")\n",
    "    print(f\"Total Views: {total_views:,}\")\n",
    "    print(f\"Total Likes: {total_likes:,}\")\n",
    "    print(f\"Total Comments: {total_comments:,}\")\n",
    "    print(f\"\\nAverage Views per Video: {avg_views:,.2f}\")\n",
    "    print(f\"Average Likes per Video: {avg_likes:,.2f}\")\n",
    "    print(f\"Average Comments per Video: {avg_comments:,.2f}\")\n",
    "\n",
    "def fetch_and_translate_transcripts(video_ids, target_language):\n",
    "    transcripts = {}\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            # Fetch transcript\n",
    "            available_transcripts = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "            transcript = None\n",
    "            if target_language in [t.language_code for t in available_transcripts]:\n",
    "                transcript = available_transcripts.find_transcript([target_language])\n",
    "            elif \"fr\" in [t.language_code for t in available_transcripts]:\n",
    "                transcript = available_transcripts.find_generated_transcript([\"fr\"])\n",
    "            else:\n",
    "                print(f\"No suitable transcript available for video ID: {video_id}\")\n",
    "                continue\n",
    "            # Fetch and translate\n",
    "            data = transcript.fetch()\n",
    "            translated_data = [\n",
    "                {\n",
    "                    \"time\": f\"{entry['start']:.2f}\",\n",
    "                    \"original\": entry[\"text\"],\n",
    "                    \"translated\": translator.translate(entry[\"text\"], src=\"fr\", dest=target_language).text\n",
    "                }\n",
    "                for entry in data\n",
    "            ]\n",
    "            transcripts[video_id] = translated_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video ID {video_id}: {e}\")\n",
    "    return transcripts\n",
    "\n",
    "def save_transcripts_to_file(transcripts):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for video_id, data in transcripts.items():\n",
    "            f.write(f\"Video ID: {video_id}\\n\")\n",
    "            f.write(f\"{'Time (s)':<10} | {'Original':<40} | {'Translated'}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            for entry in data:\n",
    "                f.write(f\"{entry['time']:<10} | {entry['original']:<40} | {entry['translated']}\\n\")\n",
    "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "def analyze_sentiment(transcripts):\n",
    "    sentiments = []\n",
    "    for video_id, data in transcripts.items():\n",
    "        for entry in data:\n",
    "            score = sia.polarity_scores(entry[\"translated\"])\n",
    "            sentiments.append(score[\"compound\"])\n",
    "    print(\"Average Sentiment Score:\", sum(sentiments) / len(sentiments) if sentiments else \"No data available.\")\n",
    "\n",
    "def extract_keywords(transcripts):\n",
    "    all_text = \" \".join(entry[\"translated\"] for data in transcripts.values() for entry in data)\n",
    "    word_counts = Counter(all_text.split())\n",
    "    print(\"Top 10 Keywords:\")\n",
    "    for word, count in word_counts.most_common(10):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "def generate_wordcloud(transcripts):\n",
    "    all_text = \" \".join(entry[\"translated\"] for data in transcripts.values() for entry in data)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(all_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def summarize_transcripts(transcripts):\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    all_text = \" \".join(entry[\"translated\"] for data in transcripts.values() for entry in data)\n",
    "    try:\n",
    "        summary = summarizer(all_text, max_length=130, min_length=30, do_sample=False)\n",
    "        print(\"Summary:\\n\", summary[0]['summary_text'])\n",
    "    except Exception as e:\n",
    "        print(\"Unable to summarize:\", e)\n",
    "\n",
    "def model_topics(transcripts, n_topics=3):\n",
    "    all_text = [\" \".join(entry[\"translated\"] for entry in data) for data in transcripts.values()]\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    doc_term_matrix = vectorizer.fit_transform(all_text)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "    for i, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic {i + 1}:\")\n",
    "        words = [vectorizer.get_feature_names_out()[index] for index in topic.argsort()[-10:]]\n",
    "        print(\" \".join(words))\n",
    "\n",
    "def pacing_analysis(transcripts):\n",
    "    for video_id, data in transcripts.items():\n",
    "        times = [float(entry[\"time\"]) for entry in data]\n",
    "        intervals = [times[i + 1] - times[i] for i in range(len(times) - 1)]\n",
    "        print(f\"Video ID: {video_id}\")\n",
    "        if intervals:\n",
    "            print(f\"Average Interval: {sum(intervals) / len(intervals):.2f} seconds\")\n",
    "        else:\n",
    "            print(\"No intervals available.\")\n",
    "\n",
    "def interactive_menu():\n",
    "    video_ids = input(\"Enter YouTube video IDs separated by commas: \").strip().split(\",\")\n",
    "    target_language = input(\"Enter the target language code (e.g., 'en' for English): \").strip()\n",
    "\n",
    "    # Fetch transcripts\n",
    "    transcripts = fetch_and_translate_transcripts(video_ids, target_language)\n",
    "    save_transcripts_to_file(transcripts)\n",
    "\n",
    "    # Fetch video metadata\n",
    "    video_metadata = fetch_video_metadata(video_ids)\n",
    "    save_metadata_to_file(video_metadata)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nAnalysis Options:\")\n",
    "        print(\"1. Sentiment Analysis\")\n",
    "        print(\"2. Keyword Extraction\")\n",
    "        print(\"3. Generate WordCloud\")\n",
    "        print(\"4. Summarize Transcripts\")\n",
    "        print(\"5. Topic Modeling\")\n",
    "        print(\"6. Pacing Analysis\")\n",
    "        print(\"7. Video Metrics Analysis\")\n",
    "        print(\"8. Exit\")\n",
    "        choice = input(\"Select an option (1-8): \").strip()\n",
    "\n",
    "        if choice == \"1\":\n",
    "            analyze_sentiment(transcripts)\n",
    "        elif choice == \"2\":\n",
    "            extract_keywords(transcripts)\n",
    "        elif choice == \"3\":\n",
    "            generate_wordcloud(transcripts)\n",
    "        elif choice == \"4\":\n",
    "            summarize_transcripts(transcripts)\n",
    "        elif choice == \"5\":\n",
    "            n_topics = int(input(\"Enter the number of topics to model (default 3): \") or 3)\n",
    "            model_topics(transcripts, n_topics)\n",
    "        elif choice == \"6\":\n",
    "            pacing_analysis(transcripts)\n",
    "        elif choice == \"7\":\n",
    "            analyze_video_metrics(video_metadata)\n",
    "        elif choice == \"8\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "def advanced_metadata_analysis(video_metadata):\n",
    "    \"\"\"\n",
    "    Perform advanced analysis on video metadata\n",
    "    \n",
    "    Args:\n",
    "        video_metadata (dict): Dictionary of video metadata\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Advanced Metadata Analysis ---\")\n",
    "    \n",
    "    # Channel Analysis\n",
    "    channel_counts = {}\n",
    "    for metadata in video_metadata.values():\n",
    "        channel = metadata['channel_title']\n",
    "        channel_counts[channel] = channel_counts.get(channel, 0) + 1\n",
    "    \n",
    "    print(\"\\nChannel Distribution:\")\n",
    "    for channel, count in sorted(channel_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{channel}: {count} video(s)\")\n",
    "    \n",
    "    # Temporal Analysis\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Convert published dates to datetime\n",
    "    published_dates = [\n",
    "        datetime.fromisoformat(metadata['published_at'].replace('Z', '+00:00')) \n",
    "        for metadata in video_metadata.values() \n",
    "        if metadata['published_at'] != 'N/A'\n",
    "    ]\n",
    "    \n",
    "    if published_dates:\n",
    "        df_dates = pd.DataFrame({'published_at': published_dates})\n",
    "        print(\"\\nPublished Date Analysis:\")\n",
    "        print(\"Earliest Video:\", df_dates['published_at'].min())\n",
    "        print(\"Latest Video:\", df_dates['published_at'].max())\n",
    "        \n",
    "        # Monthly distribution\n",
    "        df_dates['month'] = df_dates['published_at'].dt.to_period('M')\n",
    "        monthly_dist = df_dates['month'].value_counts().sort_index()\n",
    "        print(\"\\nMonthly Video Distribution:\")\n",
    "        for month, count in monthly_dist.items():\n",
    "            print(f\"{month}: {count} video(s)\")\n",
    "    \n",
    "    # Engagement Analysis\n",
    "    engagement_metrics = pd.DataFrame.from_records([\n",
    "        {\n",
    "            'video_id': video_id, \n",
    "            'title': metadata['title'], \n",
    "            'views': metadata['view_count'], \n",
    "            'likes': metadata['like_count'], \n",
    "            'comments': metadata['comment_count']\n",
    "        } \n",
    "        for video_id, metadata in video_metadata.items()\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nEngagement Metrics:\")\n",
    "    print(\"Top 5 Videos by Views:\")\n",
    "    print(engagement_metrics.nlargest(5, 'views')[['title', 'views']])\n",
    "    \n",
    "    print(\"\\nTop 5 Videos by Likes:\")\n",
    "    print(engagement_metrics.nlargest(5, 'likes')[['title', 'likes']])\n",
    "    \n",
    "    print(\"\\nTop 5 Videos by Comments:\")\n",
    "    print(engagement_metrics.nlargest(5, 'comments')[['title', 'comments']])\n",
    "    \n",
    "    # Engagement Ratio Analysis\n",
    "    engagement_metrics['like_rate'] = engagement_metrics['likes'] / engagement_metrics['views'] * 100\n",
    "    engagement_metrics['comment_rate'] = engagement_metrics['comments'] / engagement_metrics['views'] * 100\n",
    "    \n",
    "    print(\"\\nAverage Engagement Rates:\")\n",
    "    print(f\"Average Like Rate: {engagement_metrics['like_rate'].mean():.2f}%\")\n",
    "    print(f\"Average Comment Rate: {engagement_metrics['comment_rate'].mean():.2f}%\")\n",
    "    \n",
    "    # Tag Analysis\n",
    "    all_tags = []\n",
    "    for metadata in video_metadata.values():\n",
    "        all_tags.extend(metadata.get('tags', []))\n",
    "    \n",
    "    tag_counts = Counter(all_tags)\n",
    "    print(\"\\nTop 10 Tags:\")\n",
    "    for tag, count in tag_counts.most_common(10):\n",
    "        print(f\"{tag}: {count}\")\n",
    "    \n",
    "    # Privacy Status Distribution\n",
    "    privacy_status = Counter(\n",
    "        metadata['privacy_status'] for metadata in video_metadata.values()\n",
    "    )\n",
    "    print(\"\\nPrivacy Status Distribution:\")\n",
    "    for status, count in privacy_status.items():\n",
    "        print(f\"{status}: {count} video(s)\")\n",
    "\n",
    "def comment_sentiment_analysis(video_metadata):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on video comments\n",
    "    \n",
    "    Args:\n",
    "        video_metadata (dict): Dictionary of video metadata\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Comment Sentiment Analysis ---\")\n",
    "    \n",
    "    # Reuse existing sentiment analyzer\n",
    "    try:\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing sentiment analyzer: {e}\")\n",
    "        return\n",
    "    \n",
    "    all_comment_sentiments = []\n",
    "    \n",
    "    for video_id, metadata in video_metadata.items():\n",
    "        comments = metadata.get('comments', [])\n",
    "        if not comments:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nAnalysis for Video: {metadata['title']}\")\n",
    "        comment_sentiments = []\n",
    "        \n",
    "        for comment in comments:\n",
    "            sentiment_score = sia.polarity_scores(comment['text'])\n",
    "            comment_sentiments.append(sentiment_score['compound'])\n",
    "        \n",
    "        if comment_sentiments:\n",
    "            avg_sentiment = sum(comment_sentiments) / len(comment_sentiments)\n",
    "            all_comment_sentiments.extend(comment_sentiments)\n",
    "            \n",
    "            print(f\"Number of comments analyzed: {len(comments)}\")\n",
    "            print(f\"Average Comment Sentiment: {avg_sentiment:.2f}\")\n",
    "            print(\"Sentiment Categories:\")\n",
    "            print(f\"Positive Comments: {sum(1 for s in comment_sentiments if s > 0)}\")\n",
    "            print(f\"Neutral Comments: {sum(1 for s in comment_sentiments if s == 0)}\")\n",
    "            print(f\"Negative Comments: {sum(1 for s in comment_sentiments if s < 0)}\")\n",
    "    \n",
    "    if all_comment_sentiments:\n",
    "        overall_avg_sentiment = sum(all_comment_sentiments) / len(all_comment_sentiments)\n",
    "        print(f\"\\nOverall Average Sentiment Across All Videos: {overall_avg_sentiment:.2f}\")\n",
    "\n",
    "# Modify the existing interactive_menu function\n",
    "def interactive_menu():\n",
    "    video_ids = input(\"Enter YouTube video IDs separated by commas: \").strip().split(\",\")\n",
    "    target_language = input(\"Enter the target language code (e.g., 'en' for English): \").strip()\n",
    "\n",
    "    # Fetch transcripts\n",
    "    transcripts = fetch_and_translate_transcripts(video_ids, target_language)\n",
    "    save_transcripts_to_file(transcripts)\n",
    "\n",
    "    # Fetch video metadata\n",
    "    video_metadata = fetch_video_metadata(video_ids)\n",
    "    save_metadata_to_file(video_metadata)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nAnalysis Options:\")\n",
    "        print(\"1. Sentiment Analysis\")\n",
    "        print(\"2. Keyword Extraction\")\n",
    "        print(\"3. Generate WordCloud\")\n",
    "        print(\"4. Summarize Transcripts\")\n",
    "        print(\"5. Topic Modeling\")\n",
    "        print(\"6. Pacing Analysis\")\n",
    "        print(\"7. Basic Video Metrics Analysis\")\n",
    "        print(\"8. Advanced Metadata Analysis\")\n",
    "        print(\"9. Comment Sentiment Analysis\")\n",
    "        print(\"10. Exit\")\n",
    "        choice = input(\"Select an option (1-10): \").strip()\n",
    "\n",
    "        if choice == \"1\":\n",
    "            analyze_sentiment(transcripts)\n",
    "        elif choice == \"2\":\n",
    "            extract_keywords(transcripts)\n",
    "        elif choice == \"3\":\n",
    "            generate_wordcloud(transcripts)\n",
    "        elif choice == \"4\":\n",
    "            summarize_transcripts(transcripts)\n",
    "        elif choice == \"5\":\n",
    "            n_topics = int(input(\"Enter the number of topics to model (default 3): \") or 3)\n",
    "            model_topics(transcripts, n_topics)\n",
    "        elif choice == \"6\":\n",
    "            pacing_analysis(transcripts)\n",
    "        elif choice == \"7\":\n",
    "            analyze_video_metrics(video_metadata)\n",
    "        elif choice == \"8\":\n",
    "            advanced_metadata_analysis(video_metadata)\n",
    "        elif choice == \"9\":\n",
    "            comment_sentiment_analysis(video_metadata)\n",
    "        elif choice == \"10\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "import os\n",
    "import sys\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from googletrans import Translator\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "translator = Translator()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "output_file = \"transcript_results.txt\"\n",
    "metadata_file = \"video_metadata.csv\"\n",
    "\n",
    "# YouTube Data API setup\n",
    "# Replace with your actual API key\n",
    "DEVELOPER_KEY = \"AIzaSyDoDfIPqJNhh2CvULX7FNqMmvjQv9G0qUo\"\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "# [All previous functions remain the same until the end of the script]\n",
    "\n",
    "def visualize_engagement_metrics(video_metadata):\n",
    "    \"\"\"\n",
    "    Create visualizations for video engagement metrics\n",
    "    \n",
    "    Args:\n",
    "        video_metadata (dict): Dictionary of video metadata\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    df = pd.DataFrame.from_records([\n",
    "        {\n",
    "            'video_id': video_id, \n",
    "            'title': metadata['title'], \n",
    "            'views': metadata['view_count'], \n",
    "            'likes': metadata['like_count'], \n",
    "            'comments': metadata['comment_count']\n",
    "        } \n",
    "        for video_id, metadata in video_metadata.items()\n",
    "    ])\n",
    "    \n",
    "    # Set up the plots\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Views Comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    df.sort_values('views', ascending=False).plot(\n",
    "        x='title', \n",
    "        y='views', \n",
    "        kind='bar', \n",
    "        ax=plt.gca(), \n",
    "        rot=45, \n",
    "        title='Video Views Comparison'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Likes Comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    df.sort_values('likes', ascending=False).plot(\n",
    "        x='title', \n",
    "        y='likes', \n",
    "        kind='bar', \n",
    "        ax=plt.gca(), \n",
    "        rot=45, \n",
    "        title='Video Likes Comparison'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Engagement Rate\n",
    "    plt.subplot(2, 2, 3)\n",
    "    df['like_rate'] = df['likes'] / df['views'] * 100\n",
    "    df['comment_rate'] = df['comments'] / df['views'] * 100\n",
    "    \n",
    "    engagement_rates = df[['title', 'like_rate', 'comment_rate']].melt(\n",
    "        id_vars='title', \n",
    "        var_name='Rate Type', \n",
    "        value_name='Percentage'\n",
    "    )\n",
    "    \n",
    "    sns.barplot(\n",
    "        x='title', \n",
    "        y='Percentage', \n",
    "        hue='Rate Type', \n",
    "        data=engagement_rates\n",
    "    )\n",
    "    plt.title('Engagement Rates')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def compare_video_topics(transcripts, n_topics=3):\n",
    "    \"\"\"\n",
    "    Compare topics across multiple videos\n",
    "    \n",
    "    Args:\n",
    "        transcripts (dict): Dictionary of video transcripts\n",
    "        n_topics (int): Number of topics to extract\n",
    "    \"\"\"\n",
    "    # Prepare texts\n",
    "    all_texts = [\" \".join(entry[\"translated\"] for entry in data) for data in transcripts.values()]\n",
    "    \n",
    "    # Use TF-IDF Vectorizer for better topic representation\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # Topic Modeling\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(tfidf_matrix)\n",
    "    \n",
    "    print(\"\\nTopic Comparisons Across Videos:\")\n",
    "    for i, topic in enumerate(lda.components_):\n",
    "        print(f\"\\nTopic {i + 1}:\")\n",
    "        # Get top words for each topic\n",
    "        top_words_indices = topic.argsort()[-10:][::-1]\n",
    "        top_words = [vectorizer.get_feature_names_out()[idx] for idx in top_words_indices]\n",
    "        print(\" \".join(top_words))\n",
    "    \n",
    "    # Compute topic similarity\n",
    "    topic_vectors = lda.components_\n",
    "    similarity_matrix = cosine_similarity(topic_vectors)\n",
    "    \n",
    "    print(\"\\nTopic Similarity Matrix:\")\n",
    "    print(similarity_matrix)\n",
    "    \n",
    "    # Visualize topic similarity\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        similarity_matrix, \n",
    "        annot=True, \n",
    "        cmap='YlGnBu', \n",
    "        xticklabels=[f'Topic {i+1}' for i in range(n_topics)],\n",
    "        yticklabels=[f'Topic {i+1}' for i in range(n_topics)]\n",
    "    )\n",
    "    plt.title('Topic Similarity Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_video_similarities(transcripts):\n",
    "    \"\"\"\n",
    "    Analyze similarities between video transcripts\n",
    "    \n",
    "    Args:\n",
    "        transcripts (dict): Dictionary of video transcripts\n",
    "    \"\"\"\n",
    "    # Prepare texts\n",
    "    all_texts = [\" \".join(entry[\"translated\"] for entry in data) for data in transcripts.values()]\n",
    "    video_ids = list(transcripts.keys())\n",
    "    \n",
    "    # Use TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    similarity_df = pd.DataFrame(\n",
    "        similarity_matrix, \n",
    "        index=video_ids, \n",
    "        columns=video_ids\n",
    "    )\n",
    "    \n",
    "    print(\"\\nVideo Transcript Similarity Matrix:\")\n",
    "    print(similarity_df)\n",
    "    \n",
    "    # Visualize similarity\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        similarity_df, \n",
    "        annot=True, \n",
    "        cmap='YlGnBu', \n",
    "        xticklabels=video_ids,\n",
    "        yticklabels=video_ids\n",
    "    )\n",
    "    plt.title('Video Transcript Similarity Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def cross_video_keyword_analysis(transcripts):\n",
    "    \"\"\"\n",
    "    Perform cross-video keyword analysis\n",
    "    \n",
    "    Args:\n",
    "        transcripts (dict): Dictionary of video transcripts\n",
    "    \"\"\"\n",
    "    # Prepare texts for each video\n",
    "    video_texts = {\n",
    "        video_id: \" \".join(entry[\"translated\"] for entry in data)\n",
    "        for video_id, data in transcripts.items()\n",
    "    }\n",
    "    \n",
    "    # Create TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(video_texts.values())\n",
    "    \n",
    "    # Get feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Analyze top keywords for each video\n",
    "    print(\"\\nTop Keywords per Video:\")\n",
    "    for video_id, tfidf_vector in zip(video_texts.keys(), tfidf_matrix):\n",
    "        # Get top 10 keywords\n",
    "        top_tfidf_indices = tfidf_vector.toarray()[0].argsort()[-10:][::-1]\n",
    "        top_keywords = [feature_names[idx] for idx in top_tfidf_indices]\n",
    "        print(f\"\\nVideo {video_id}:\")\n",
    "        print(\", \".join(top_keywords))\n",
    "    \n",
    "    # Common keywords across videos\n",
    "    tfidf_dense = tfidf_matrix.toarray()\n",
    "    global_keyword_scores = tfidf_dense.mean(axis=0)\n",
    "    \n",
    "    # Get top global keywords\n",
    "    top_global_keyword_indices = global_keyword_scores.argsort()[-10:][::-1]\n",
    "    top_global_keywords = [feature_names[idx] for idx in top_global_keyword_indices]\n",
    "    \n",
    "    print(\"\\nTop Global Keywords:\")\n",
    "    print(\", \".join(top_global_keywords))\n",
    "\n",
    "def interactive_menu():\n",
    "    video_ids = input(\"Enter YouTube video IDs separated by commas: \").strip().split(\",\")\n",
    "    target_language = input(\"Enter the target language code (e.g., 'en' for English): \").strip()\n",
    "\n",
    "    # Fetch transcripts\n",
    "    transcripts = fetch_and_translate_transcripts(video_ids, target_language)\n",
    "    save_transcripts_to_file(transcripts)\n",
    "\n",
    "    # Fetch video metadata\n",
    "    video_metadata = fetch_video_metadata(video_ids)\n",
    "    save_metadata_to_file(video_metadata)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nAnalysis Options:\")\n",
    "        print(\"1. Sentiment Analysis\")\n",
    "        print(\"2. Keyword Extraction\")\n",
    "        print(\"3. Generate WordCloud\")\n",
    "        print(\"4. Summarize Transcripts\")\n",
    "        print(\"5. Topic Modeling\")\n",
    "        print(\"6. Pacing Analysis\")\n",
    "        print(\"7. Basic Video Metrics Analysis\")\n",
    "        print(\"8. Advanced Metadata Analysis\")\n",
    "        print(\"9. Comment Sentiment Analysis\")\n",
    "        print(\"10. Visualize Engagement Metrics\")\n",
    "        print(\"11. Compare Video Topics\")\n",
    "        print(\"12. Analyze Video Similarities\")\n",
    "        print(\"13. Cross-Video Keyword Analysis\")\n",
    "        print(\"14. Exit\")\n",
    "        choice = input(\"Select an option (1-14): \").strip()\n",
    "\n",
    "        if choice == \"1\":\n",
    "            analyze_sentiment(transcripts)\n",
    "        elif choice == \"2\":\n",
    "            extract_keywords(transcripts)\n",
    "        elif choice == \"3\":\n",
    "            generate_wordcloud(transcripts)\n",
    "        elif choice == \"4\":\n",
    "            summarize_transcripts(transcripts)\n",
    "        elif choice == \"5\":\n",
    "            n_topics = int(input(\"Enter the number of topics to model (default 3): \") or 3)\n",
    "            model_topics(transcripts, n_topics)\n",
    "        elif choice == \"6\":\n",
    "            pacing_analysis(transcripts)\n",
    "        elif choice == \"7\":\n",
    "            analyze_video_metrics(video_metadata)\n",
    "        elif choice == \"8\":\n",
    "            advanced_metadata_analysis(video_metadata)\n",
    "        elif choice == \"9\":\n",
    "            comment_sentiment_analysis(video_metadata)\n",
    "        elif choice == \"10\":\n",
    "            visualize_engagement_metrics(video_metadata)\n",
    "        elif choice == \"11\":\n",
    "            n_topics = int(input(\"Enter the number of topics to model (default 3): \") or 3)\n",
    "            compare_video_topics(transcripts, n_topics)\n",
    "        elif choice == \"12\":\n",
    "            analyze_video_similarities(transcripts)\n",
    "        elif choice == \"13\":\n",
    "            cross_video_keyword_analysis(transcripts)\n",
    "        elif choice == \"14\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_menu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aivancity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
