{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from googletrans import Translator\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "translator = Translator()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "output_file = \"transcript_results.txt\"\n",
    "metadata_file = \"video_metadata.csv\"\n",
    "\n",
    "# YouTube Data API setup\n",
    "# Replace with your actual API key\n",
    "DEVELOPER_KEY = \"\"\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "def get_youtube_service():\n",
    "    \"\"\"Initialize YouTube API client\"\"\"\n",
    "    try:\n",
    "        youtube = googleapiclient.discovery.build(\n",
    "            YOUTUBE_API_SERVICE_NAME, \n",
    "            YOUTUBE_API_VERSION, \n",
    "            developerKey=DEVELOPER_KEY\n",
    "        )\n",
    "        return youtube\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing YouTube API: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def fetch_video_metadata(video_ids):\n",
    "    \"\"\"\n",
    "    Retrieve detailed metadata for given YouTube video IDs\n",
    "    \n",
    "    Args:\n",
    "        video_ids (list): List of YouTube video IDs\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of video metadata\n",
    "    \"\"\"\n",
    "    youtube = get_youtube_service()\n",
    "    video_metadata = {}\n",
    "\n",
    "    try:\n",
    "        # Retrieve video details\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,statistics,contentDetails,status\",\n",
    "            id=\",\".join(video_ids)\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for video in response.get('items', []):\n",
    "            video_id = video['id']\n",
    "            metadata = {\n",
    "                'title': video['snippet'].get('title', 'N/A'),\n",
    "                'channel_title': video['snippet'].get('channelTitle', 'N/A'),\n",
    "                'published_at': video['snippet'].get('publishedAt', 'N/A'),\n",
    "                'description': video['snippet'].get('description', 'N/A'),\n",
    "                'tags': video['snippet'].get('tags', []),\n",
    "                'view_count': int(video['statistics'].get('viewCount', 0)),\n",
    "                'like_count': int(video['statistics'].get('likeCount', 0)),\n",
    "                'comment_count': int(video['statistics'].get('commentCount', 0)),\n",
    "                'duration': video['contentDetails'].get('duration', 'N/A'),\n",
    "                'privacy_status': video['status'].get('privacyStatus', 'N/A')\n",
    "            }\n",
    "            video_metadata[video_id] = metadata\n",
    "\n",
    "        # Fetch and add comments (limited to most recent comments)\n",
    "        comments_metadata = fetch_video_comments(youtube, video_ids)\n",
    "        for video_id, comments in comments_metadata.items():\n",
    "            video_metadata[video_id]['comments'] = comments\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching video metadata: {e}\")\n",
    "\n",
    "    return video_metadata\n",
    "\n",
    "def fetch_video_comments(youtube, video_ids, max_comments_per_video=10):\n",
    "    \"\"\"\n",
    "    Retrieve comments for given video IDs\n",
    "    \n",
    "    Args:\n",
    "        youtube (obj): YouTube API service object\n",
    "        video_ids (list): List of YouTube video IDs\n",
    "        max_comments_per_video (int): Maximum number of comments to retrieve per video\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of comments for each video\n",
    "    \"\"\"\n",
    "    comments_metadata = {}\n",
    "\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            comments_request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=max_comments_per_video,\n",
    "                order=\"relevance\"\n",
    "            )\n",
    "            comments_response = comments_request.execute()\n",
    "\n",
    "            comments = [\n",
    "                {\n",
    "                    'text': comment['snippet']['topLevelComment']['snippet']['textDisplay'],\n",
    "                    'author': comment['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "                    'like_count': comment['snippet']['topLevelComment']['snippet'].get('likeCount', 0),\n",
    "                    'published_at': comment['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "                }\n",
    "                for comment in comments_response.get('items', [])\n",
    "            ]\n",
    "            comments_metadata[video_id] = comments\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "            comments_metadata[video_id] = []\n",
    "\n",
    "    return comments_metadata\n",
    "\n",
    "def save_metadata_to_file(video_metadata):\n",
    "    \"\"\"\n",
    "    Save video metadata to a CSV file\n",
    "    \n",
    "    Args:\n",
    "        video_metadata (dict): Dictionary of video metadata\n",
    "    \"\"\"\n",
    "    # Prepare data for DataFrame\n",
    "    metadata_list = []\n",
    "    for video_id, metadata in video_metadata.items():\n",
    "        row = {\n",
    "            'video_id': video_id,\n",
    "            **{k: v for k, v in metadata.items() if k not in ['comments', 'tags']}\n",
    "        }\n",
    "        metadata_list.append(row)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(metadata_list)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(metadata_file, index=False, encoding='utf-8')\n",
    "    print(f\"Metadata saved to {metadata_file}\")\n",
    "\n",
    "def analyze_video_metrics(video_metadata):\n",
    "    \"\"\"\n",
    "    Provide basic analysis of video metrics\n",
    "    \n",
    "    Args:\n",
    "        video_metadata (dict): Dictionary of video metadata\n",
    "    \"\"\"\n",
    "    print(\"\\nVideo Metrics Analysis:\")\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    total_views = sum(metadata['view_count'] for metadata in video_metadata.values())\n",
    "    total_likes = sum(metadata['like_count'] for metadata in video_metadata.values())\n",
    "    total_comments = sum(metadata['comment_count'] for metadata in video_metadata.values())\n",
    "    \n",
    "    # Calculate averages\n",
    "    video_count = len(video_metadata)\n",
    "    avg_views = total_views / video_count\n",
    "    avg_likes = total_likes / video_count\n",
    "    avg_comments = total_comments / video_count\n",
    "    \n",
    "    print(f\"Total Videos: {video_count}\")\n",
    "    print(f\"Total Views: {total_views:,}\")\n",
    "    print(f\"Total Likes: {total_likes:,}\")\n",
    "    print(f\"Total Comments: {total_comments:,}\")\n",
    "    print(f\"\\nAverage Views per Video: {avg_views:,.2f}\")\n",
    "    print(f\"Average Likes per Video: {avg_likes:,.2f}\")\n",
    "    print(f\"Average Comments per Video: {avg_comments:,.2f}\")\n",
    "\n",
    "def fetch_and_translate_transcripts(video_ids, target_language):\n",
    "    transcripts = {}\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            # Fetch transcript\n",
    "            available_transcripts = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "            transcript = None\n",
    "            if target_language in [t.language_code for t in available_transcripts]:\n",
    "                transcript = available_transcripts.find_transcript([target_language])\n",
    "            elif \"fr\" in [t.language_code for t in available_transcripts]:\n",
    "                transcript = available_transcripts.find_generated_transcript([\"fr\"])\n",
    "            else:\n",
    "                print(f\"No suitable transcript available for video ID: {video_id}\")\n",
    "                continue\n",
    "            # Fetch and translate\n",
    "            data = transcript.fetch()\n",
    "            translated_data = [\n",
    "                {\n",
    "                    \"time\": f\"{entry['start']:.2f}\",\n",
    "                    \"original\": entry[\"text\"],\n",
    "                    \"translated\": translator.translate(entry[\"text\"], src=\"fr\", dest=target_language).text\n",
    "                }\n",
    "                for entry in data\n",
    "            ]\n",
    "            transcripts[video_id] = translated_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video ID {video_id}: {e}\")\n",
    "    return transcripts\n",
    "\n",
    "def save_transcripts_to_file(transcripts):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for video_id, data in transcripts.items():\n",
    "            f.write(f\"Video ID: {video_id}\\n\")\n",
    "            f.write(f\"{'Time (s)':<10} | {'Original':<40} | {'Translated'}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            for entry in data:\n",
    "                f.write(f\"{entry['time']:<10} | {entry['original']:<40} | {entry['translated']}\\n\")\n",
    "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "def analyze_sentiment(transcripts):\n",
    "    sentiments = []\n",
    "    for video_id, data in transcripts.items():\n",
    "        for entry in data:\n",
    "            score = sia.polarity_scores(entry[\"translated\"])\n",
    "            sentiments.append(score[\"compound\"])\n",
    "    print(\"Average Sentiment Score:\", sum(sentiments) / len(sentiments) if sentiments else \"No data available.\")\n",
    "\n",
    "def extract_keywords(transcripts):\n",
    "    all_text = \" \".join(entry[\"translated\"] for data in transcripts.values() for entry in data)\n",
    "    word_counts = Counter(all_text.split())\n",
    "    print(\"Top 10 Keywords:\")\n",
    "    for word, count in word_counts.most_common(10):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "def generate_wordcloud(transcripts):\n",
    "    all_text = \" \".join(entry[\"translated\"] for data in transcripts.values() for entry in data)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(all_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def summarize_transcripts(transcripts):\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    all_text = \" \".join(entry[\"translated\"] for data in transcripts.values() for entry in data)\n",
    "    try:\n",
    "        summary = summarizer(all_text, max_length=130, min_length=30, do_sample=False)\n",
    "        print(\"Summary:\\n\", summary[0]['summary_text'])\n",
    "    except Exception as e:\n",
    "        print(\"Unable to summarize:\", e)\n",
    "\n",
    "def model_topics(transcripts, n_topics=3):\n",
    "    all_text = [\" \".join(entry[\"translated\"] for entry in data) for data in transcripts.values()]\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    doc_term_matrix = vectorizer.fit_transform(all_text)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "    for i, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic {i + 1}:\")\n",
    "        words = [vectorizer.get_feature_names_out()[index] for index in topic.argsort()[-10:]]\n",
    "        print(\" \".join(words))\n",
    "\n",
    "def pacing_analysis(transcripts):\n",
    "    for video_id, data in transcripts.items():\n",
    "        times = [float(entry[\"time\"]) for entry in data]\n",
    "        intervals = [times[i + 1] - times[i] for i in range(len(times) - 1)]\n",
    "        print(f\"Video ID: {video_id}\")\n",
    "        if intervals:\n",
    "            print(f\"Average Interval: {sum(intervals) / len(intervals):.2f} seconds\")\n",
    "        else:\n",
    "            print(\"No intervals available.\")\n",
    "\n",
    "def interactive_menu():\n",
    "    video_ids = input(\"Enter YouTube video IDs separated by commas: \").strip().split(\",\")\n",
    "    target_language = input(\"Enter the target language code (e.g., 'en' for English): \").strip()\n",
    "\n",
    "    # Fetch transcripts\n",
    "    transcripts = fetch_and_translate_transcripts(video_ids, target_language)\n",
    "    save_transcripts_to_file(transcripts)\n",
    "\n",
    "    # Fetch video metadata\n",
    "    video_metadata = fetch_video_metadata(video_ids)\n",
    "    save_metadata_to_file(video_metadata)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nAnalysis Options:\")\n",
    "        print(\"1. Sentiment Analysis\")\n",
    "        print(\"2. Keyword Extraction\")\n",
    "        print(\"3. Generate WordCloud\")\n",
    "        print(\"4. Summarize Transcripts\")\n",
    "        print(\"5. Topic Modeling\")\n",
    "        print(\"6. Pacing Analysis\")\n",
    "        print(\"7. Video Metrics Analysis\")\n",
    "        print(\"8. Exit\")\n",
    "        choice = input(\"Select an option (1-8): \").strip()\n",
    "\n",
    "        if choice == \"1\":\n",
    "            analyze_sentiment(transcripts)\n",
    "        elif choice == \"2\":\n",
    "            extract_keywords(transcripts)\n",
    "        elif choice == \"3\":\n",
    "            generate_wordcloud(transcripts)\n",
    "        elif choice == \"4\":\n",
    "            summarize_transcripts(transcripts)\n",
    "        elif choice == \"5\":\n",
    "            n_topics = int(input(\"Enter the number of topics to model (default 3): \") or 3)\n",
    "            model_topics(transcripts, n_topics)\n",
    "        elif choice == \"6\":\n",
    "            pacing_analysis(transcripts)\n",
    "        elif choice == \"7\":\n",
    "            analyze_video_metrics(video_metadata)\n",
    "        elif choice == \"8\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "def advanced_metadata_analysis(video_metadata):\n",
    "    \"\"\"\n",
    "    Perform advanced analysis on video metadata\n",
    "    \n",
    "    Args:\n",
    "        video_metadata (dict): Dictionary of video metadata\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Advanced Metadata Analysis ---\")\n",
    "    \n",
    "    # Channel Analysis\n",
    "    channel_counts = {}\n",
    "    for metadata in video_metadata.values():\n",
    "        channel = metadata['channel_title']\n",
    "        channel_counts[channel] = channel_counts.get(channel, 0) + 1\n",
    "    \n",
    "    print(\"\\nChannel Distribution:\")\n",
    "    for channel, count in sorted(channel_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{channel}: {count} video(s)\")\n",
    "    \n",
    "    # Temporal Analysis\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Convert published dates to datetime\n",
    "    published_dates = [\n",
    "        datetime.fromisoformat(metadata['published_at'].replace('Z', '+00:00')) \n",
    "        for metadata in video_metadata.values() \n",
    "        if metadata['published_at'] != 'N/A'\n",
    "    ]\n",
    "    \n",
    "    if published_dates:\n",
    "        df_dates = pd.DataFrame({'published_at': published_dates})\n",
    "        print(\"\\nPublished Date Analysis:\")\n",
    "        print(\"Earliest Video:\", df_dates['published_at'].min())\n",
    "        print(\"Latest Video:\", df_dates['published_at'].max())\n",
    "        \n",
    "        # Monthly distribution\n",
    "        df_dates['month'] = df_dates['published_at'].dt.to_period('M')\n",
    "        monthly_dist = df_dates['month'].value_counts().sort_index()\n",
    "        print(\"\\nMonthly Video Distribution:\")\n",
    "        for month, count in monthly_dist.items():\n",
    "            print(f\"{month}: {count} video(s)\")\n",
    "    \n",
    "    # Engagement Analysis\n",
    "    engagement_metrics = pd.DataFrame.from_records([\n",
    "        {\n",
    "            'video_id': video_id, \n",
    "            'title': metadata['title'], \n",
    "            'views': metadata['view_count'], \n",
    "            'likes': metadata['like_count'], \n",
    "            'comments': metadata['comment_count']\n",
    "        } \n",
    "        for video_id, metadata in video_metadata.items()\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nEngagement Metrics:\")\n",
    "    print(\"Top 5 Videos by Views:\")\n",
    "    print(engagement_metrics.nlargest(5, 'views')[['title', 'views']])\n",
    "    \n",
    "    print(\"\\nTop 5 Videos by Likes:\")\n",
    "    print(engagement_metrics.nlargest(5, 'likes')[['title', 'likes']])\n",
    "    \n",
    "    print(\"\\nTop 5 Videos by Comments:\")\n",
    "    print(engagement_metrics.nlargest(5, 'comments')[['title', 'comments']])\n",
    "    \n",
    "    # Engagement Ratio Analysis\n",
    "    engagement_metrics['like_rate'] = engagement_metrics['likes'] / engagement_metrics['views'] * 100\n",
    "    engagement_metrics['comment_rate'] = engagement_metrics['comments'] / engagement_metrics['views'] * 100\n",
    "    \n",
    "    print(\"\\nAverage Engagement Rates:\")\n",
    "    print(f\"Average Like Rate: {engagement_metrics['like_rate'].mean():.2f}%\")\n",
    "    print(f\"Average Comment Rate: {engagement_metrics['comment_rate'].mean():.2f}%\")\n",
    "    \n",
    "    # Tag Analysis\n",
    "    all_tags = []\n",
    "    for metadata in video_metadata.values():\n",
    "        all_tags.extend(metadata.get('tags', []))\n",
    "    \n",
    "    tag_counts = Counter(all_tags)\n",
    "    print(\"\\nTop 10 Tags:\")\n",
    "    for tag, count in tag_counts.most_common(10):\n",
    "        print(f\"{tag}: {count}\")\n",
    "    \n",
    "    # Privacy Status Distribution\n",
    "    privacy_status = Counter(\n",
    "        metadata['privacy_status'] for metadata in video_metadata.values()\n",
    "    )\n",
    "    print(\"\\nPrivacy Status Distribution:\")\n",
    "    for status, count in privacy_status.items():\n",
    "        print(f\"{status}: {count} video(s)\")\n",
    "\n",
    "def comment_sentiment_analysis(video_metadata):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on video comments\n",
    "    \n",
    "    Args:\n",
    "        video_metadata (dict): Dictionary of video metadata\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Comment Sentiment Analysis ---\")\n",
    "    \n",
    "    # Reuse existing sentiment analyzer\n",
    "    try:\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing sentiment analyzer: {e}\")\n",
    "        return\n",
    "    \n",
    "    all_comment_sentiments = []\n",
    "    \n",
    "    for video_id, metadata in video_metadata.items():\n",
    "        comments = metadata.get('comments', [])\n",
    "        if not comments:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nAnalysis for Video: {metadata['title']}\")\n",
    "        comment_sentiments = []\n",
    "        \n",
    "        for comment in comments:\n",
    "            sentiment_score = sia.polarity_scores(comment['text'])\n",
    "            comment_sentiments.append(sentiment_score['compound'])\n",
    "        \n",
    "        if comment_sentiments:\n",
    "            avg_sentiment = sum(comment_sentiments) / len(comment_sentiments)\n",
    "            all_comment_sentiments.extend(comment_sentiments)\n",
    "            \n",
    "            print(f\"Number of comments analyzed: {len(comments)}\")\n",
    "            print(f\"Average Comment Sentiment: {avg_sentiment:.2f}\")\n",
    "            print(\"Sentiment Categories:\")\n",
    "            print(f\"Positive Comments: {sum(1 for s in comment_sentiments if s > 0)}\")\n",
    "            print(f\"Neutral Comments: {sum(1 for s in comment_sentiments if s == 0)}\")\n",
    "            print(f\"Negative Comments: {sum(1 for s in comment_sentiments if s < 0)}\")\n",
    "    \n",
    "    if all_comment_sentiments:\n",
    "        overall_avg_sentiment = sum(all_comment_sentiments) / len(all_comment_sentiments)\n",
    "        print(f\"\\nOverall Average Sentiment Across All Videos: {overall_avg_sentiment:.2f}\")\n",
    "\n",
    "# Modify the existing interactive_menu function\n",
    "def interactive_menu():\n",
    "    video_ids = input(\"Enter YouTube video IDs separated by commas: \").strip().split(\",\")\n",
    "    target_language = input(\"Enter the target language code (e.g., 'en' for English): \").strip()\n",
    "\n",
    "    # Fetch transcripts\n",
    "    transcripts = fetch_and_translate_transcripts(video_ids, target_language)\n",
    "    save_transcripts_to_file(transcripts)\n",
    "\n",
    "    # Fetch video metadata\n",
    "    video_metadata = fetch_video_metadata(video_ids)\n",
    "    save_metadata_to_file(video_metadata)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nAnalysis Options:\")\n",
    "        print(\"1. Sentiment Analysis\")\n",
    "        print(\"2. Keyword Extraction\")\n",
    "        print(\"3. Generate WordCloud\")\n",
    "        print(\"4. Summarize Transcripts\")\n",
    "        print(\"5. Topic Modeling\")\n",
    "        print(\"6. Pacing Analysis\")\n",
    "        print(\"7. Basic Video Metrics Analysis\")\n",
    "        print(\"8. Advanced Metadata Analysis\")\n",
    "        print(\"9. Comment Sentiment Analysis\")\n",
    "        print(\"10. Exit\")\n",
    "        choice = input(\"Select an option (1-10): \").strip()\n",
    "\n",
    "        if choice == \"1\":\n",
    "            analyze_sentiment(transcripts)\n",
    "        elif choice == \"2\":\n",
    "            extract_keywords(transcripts)\n",
    "        elif choice == \"3\":\n",
    "            generate_wordcloud(transcripts)\n",
    "        elif choice == \"4\":\n",
    "            summarize_transcripts(transcripts)\n",
    "        elif choice == \"5\":\n",
    "            n_topics = int(input(\"Enter the number of topics to model (default 3): \") or 3)\n",
    "            model_topics(transcripts, n_topics)\n",
    "        elif choice == \"6\":\n",
    "            pacing_analysis(transcripts)\n",
    "        elif choice == \"7\":\n",
    "            analyze_video_metrics(video_metadata)\n",
    "        elif choice == \"8\":\n",
    "            advanced_metadata_analysis(video_metadata)\n",
    "        elif choice == \"9\":\n",
    "            comment_sentiment_analysis(video_metadata)\n",
    "        elif choice == \"10\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "import os\n",
    "import sys\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from googletrans import Translator\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "translator = Translator()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "output_file = \"transcript_results.txt\"\n",
    "metadata_file = \"video_metadata.csv\"\n",
    "\n",
    "# YouTube Data API setup\n",
    "# Replace with your actual API key\n",
    "DEVELOPER_KEY = \"AIzaSyDoDfIPqJNhh2CvULX7FNqMmvjQv9G0qUo\"\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "# [All previous functions remain the same until the end of the script]\n",
    "\n",
    "def visualize_engagement_metrics(video_metadata):\n",
    "    \"\"\"\n",
    "    Create visualizations for video engagement metrics\n",
    "    \n",
    "    Args:\n",
    "        video_metadata (dict): Dictionary of video metadata\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    df = pd.DataFrame.from_records([\n",
    "        {\n",
    "            'video_id': video_id, \n",
    "            'title': metadata['title'], \n",
    "            'views': metadata['view_count'], \n",
    "            'likes': metadata['like_count'], \n",
    "            'comments': metadata['comment_count']\n",
    "        } \n",
    "        for video_id, metadata in video_metadata.items()\n",
    "    ])\n",
    "    \n",
    "    # Set up the plots\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Views Comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    df.sort_values('views', ascending=False).plot(\n",
    "        x='title', \n",
    "        y='views', \n",
    "        kind='bar', \n",
    "        ax=plt.gca(), \n",
    "        rot=45, \n",
    "        title='Video Views Comparison'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Likes Comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    df.sort_values('likes', ascending=False).plot(\n",
    "        x='title', \n",
    "        y='likes', \n",
    "        kind='bar', \n",
    "        ax=plt.gca(), \n",
    "        rot=45, \n",
    "        title='Video Likes Comparison'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Engagement Rate\n",
    "    plt.subplot(2, 2, 3)\n",
    "    df['like_rate'] = df['likes'] / df['views'] * 100\n",
    "    df['comment_rate'] = df['comments'] / df['views'] * 100\n",
    "    \n",
    "    engagement_rates = df[['title', 'like_rate', 'comment_rate']].melt(\n",
    "        id_vars='title', \n",
    "        var_name='Rate Type', \n",
    "        value_name='Percentage'\n",
    "    )\n",
    "    \n",
    "    sns.barplot(\n",
    "        x='title', \n",
    "        y='Percentage', \n",
    "        hue='Rate Type', \n",
    "        data=engagement_rates\n",
    "    )\n",
    "    plt.title('Engagement Rates')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def compare_video_topics(transcripts, n_topics=3):\n",
    "    \"\"\"\n",
    "    Compare topics across multiple videos\n",
    "    \n",
    "    Args:\n",
    "        transcripts (dict): Dictionary of video transcripts\n",
    "        n_topics (int): Number of topics to extract\n",
    "    \"\"\"\n",
    "    # Prepare texts\n",
    "    all_texts = [\" \".join(entry[\"translated\"] for entry in data) for data in transcripts.values()]\n",
    "    \n",
    "    # Use TF-IDF Vectorizer for better topic representation\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # Topic Modeling\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(tfidf_matrix)\n",
    "    \n",
    "    print(\"\\nTopic Comparisons Across Videos:\")\n",
    "    for i, topic in enumerate(lda.components_):\n",
    "        print(f\"\\nTopic {i + 1}:\")\n",
    "        # Get top words for each topic\n",
    "        top_words_indices = topic.argsort()[-10:][::-1]\n",
    "        top_words = [vectorizer.get_feature_names_out()[idx] for idx in top_words_indices]\n",
    "        print(\" \".join(top_words))\n",
    "    \n",
    "    # Compute topic similarity\n",
    "    topic_vectors = lda.components_\n",
    "    similarity_matrix = cosine_similarity(topic_vectors)\n",
    "    \n",
    "    print(\"\\nTopic Similarity Matrix:\")\n",
    "    print(similarity_matrix)\n",
    "    \n",
    "    # Visualize topic similarity\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        similarity_matrix, \n",
    "        annot=True, \n",
    "        cmap='YlGnBu', \n",
    "        xticklabels=[f'Topic {i+1}' for i in range(n_topics)],\n",
    "        yticklabels=[f'Topic {i+1}' for i in range(n_topics)]\n",
    "    )\n",
    "    plt.title('Topic Similarity Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_video_similarities(transcripts):\n",
    "    \"\"\"\n",
    "    Analyze similarities between video transcripts\n",
    "    \n",
    "    Args:\n",
    "        transcripts (dict): Dictionary of video transcripts\n",
    "    \"\"\"\n",
    "    # Prepare texts\n",
    "    all_texts = [\" \".join(entry[\"translated\"] for entry in data) for data in transcripts.values()]\n",
    "    video_ids = list(transcripts.keys())\n",
    "    \n",
    "    # Use TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    similarity_df = pd.DataFrame(\n",
    "        similarity_matrix, \n",
    "        index=video_ids, \n",
    "        columns=video_ids\n",
    "    )\n",
    "    \n",
    "    print(\"\\nVideo Transcript Similarity Matrix:\")\n",
    "    print(similarity_df)\n",
    "    \n",
    "    # Visualize similarity\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        similarity_df, \n",
    "        annot=True, \n",
    "        cmap='YlGnBu', \n",
    "        xticklabels=video_ids,\n",
    "        yticklabels=video_ids\n",
    "    )\n",
    "    plt.title('Video Transcript Similarity Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def cross_video_keyword_analysis(transcripts):\n",
    "    \"\"\"\n",
    "    Perform cross-video keyword analysis\n",
    "    \n",
    "    Args:\n",
    "        transcripts (dict): Dictionary of video transcripts\n",
    "    \"\"\"\n",
    "    # Prepare texts for each video\n",
    "    video_texts = {\n",
    "        video_id: \" \".join(entry[\"translated\"] for entry in data)\n",
    "        for video_id, data in transcripts.items()\n",
    "    }\n",
    "    \n",
    "    # Create TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(video_texts.values())\n",
    "    \n",
    "    # Get feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Analyze top keywords for each video\n",
    "    print(\"\\nTop Keywords per Video:\")\n",
    "    for video_id, tfidf_vector in zip(video_texts.keys(), tfidf_matrix):\n",
    "        # Get top 10 keywords\n",
    "        top_tfidf_indices = tfidf_vector.toarray()[0].argsort()[-10:][::-1]\n",
    "        top_keywords = [feature_names[idx] for idx in top_tfidf_indices]\n",
    "        print(f\"\\nVideo {video_id}:\")\n",
    "        print(\", \".join(top_keywords))\n",
    "    \n",
    "    # Common keywords across videos\n",
    "    tfidf_dense = tfidf_matrix.toarray()\n",
    "    global_keyword_scores = tfidf_dense.mean(axis=0)\n",
    "    \n",
    "    # Get top global keywords\n",
    "    top_global_keyword_indices = global_keyword_scores.argsort()[-10:][::-1]\n",
    "    top_global_keywords = [feature_names[idx] for idx in top_global_keyword_indices]\n",
    "    \n",
    "    print(\"\\nTop Global Keywords:\")\n",
    "    print(\", \".join(top_global_keywords))\n",
    "\n",
    "def interactive_menu():\n",
    "    video_ids = input(\"Enter YouTube video IDs separated by commas: \").strip().split(\",\")\n",
    "    target_language = input(\"Enter the target language code (e.g., 'en' for English): \").strip()\n",
    "\n",
    "    # Fetch transcripts\n",
    "    transcripts = fetch_and_translate_transcripts(video_ids, target_language)\n",
    "    save_transcripts_to_file(transcripts)\n",
    "\n",
    "    # Fetch video metadata\n",
    "    video_metadata = fetch_video_metadata(video_ids)\n",
    "    save_metadata_to_file(video_metadata)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nAnalysis Options:\")\n",
    "        print(\"1. Sentiment Analysis\")\n",
    "        print(\"2. Keyword Extraction\")\n",
    "        print(\"3. Generate WordCloud\")\n",
    "        print(\"4. Summarize Transcripts\")\n",
    "        print(\"5. Topic Modeling\")\n",
    "        print(\"6. Pacing Analysis\")\n",
    "        print(\"7. Basic Video Metrics Analysis\")\n",
    "        print(\"8. Advanced Metadata Analysis\")\n",
    "        print(\"9. Comment Sentiment Analysis\")\n",
    "        print(\"10. Visualize Engagement Metrics\")\n",
    "        print(\"11. Compare Video Topics\")\n",
    "        print(\"12. Analyze Video Similarities\")\n",
    "        print(\"13. Cross-Video Keyword Analysis\")\n",
    "        print(\"14. Exit\")\n",
    "        choice = input(\"Select an option (1-14): \").strip()\n",
    "\n",
    "        if choice == \"1\":\n",
    "            analyze_sentiment(transcripts)\n",
    "        elif choice == \"2\":\n",
    "            extract_keywords(transcripts)\n",
    "        elif choice == \"3\":\n",
    "            generate_wordcloud(transcripts)\n",
    "        elif choice == \"4\":\n",
    "            summarize_transcripts(transcripts)\n",
    "        elif choice == \"5\":\n",
    "            n_topics = int(input(\"Enter the number of topics to model (default 3): \") or 3)\n",
    "            model_topics(transcripts, n_topics)\n",
    "        elif choice == \"6\":\n",
    "            pacing_analysis(transcripts)\n",
    "        elif choice == \"7\":\n",
    "            analyze_video_metrics(video_metadata)\n",
    "        elif choice == \"8\":\n",
    "            advanced_metadata_analysis(video_metadata)\n",
    "        elif choice == \"9\":\n",
    "            comment_sentiment_analysis(video_metadata)\n",
    "        elif choice == \"10\":\n",
    "            visualize_engagement_metrics(video_metadata)\n",
    "        elif choice == \"11\":\n",
    "            n_topics = int(input(\"Enter the number of topics to model (default 3): \") or 3)\n",
    "            compare_video_topics(transcripts, n_topics)\n",
    "        elif choice == \"12\":\n",
    "            analyze_video_similarities(transcripts)\n",
    "        elif choice == \"13\":\n",
    "            cross_video_keyword_analysis(transcripts)\n",
    "        elif choice == \"14\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_menu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aivancity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
